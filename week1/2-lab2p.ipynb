{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a28f42de-b71e-4cbd-bd64-9a1c003c85d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting anthropic\n",
      "  Downloading anthropic-0.55.0-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (2.10.6)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from anthropic) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/lib/python3/dist-packages (from anyio<5,>=3.5.0->anthropic) (2.8)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.27.2)\n",
      "Downloading anthropic-0.55.0-py3-none-any.whl (289 kB)\n",
      "Installing collected packages: anthropic\n",
      "Successfully installed anthropic-0.55.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.12 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.12/dist-packages (8.32.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.12/dist-packages (from ipython) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/dist-packages (from ipython) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.12/dist-packages (from ipython) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from ipython) (2.18.0)\n",
      "Requirement already satisfied: stack_data in /usr/local/lib/python3.12/dist-packages (from ipython) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /usr/local/lib/python3.12/dist-packages (from ipython) (5.14.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from stack_data->ipython) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from stack_data->ipython) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.12/dist-packages (from stack_data->ipython) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from asttokens>=2.1.0->stack_data->ipython) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.12 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import ollama\n",
    "from openai import OpenAI\n",
    "!pip install anthropic\n",
    "from anthropic import Anthropic\n",
    "from dotenv import load_dotenv\n",
    "!pip install ipython\n",
    "from IPython.display import Markdown, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa58c285-098e-4e05-8158-7d476d711860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# always remember to do this\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e786b3ec-299e-413f-8d87-9a1cfbda09ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anthropic key is not valid\n",
      "the openai_api_key is valid\n"
     ]
    }
   ],
   "source": [
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "if anthropic_api_key:\n",
    "    print(\"the anthropic api key is valid!\")\n",
    "else:\n",
    "    print(\"anthropic key is not valid\")\n",
    "\n",
    "\n",
    "\n",
    "if openai_api_key:\n",
    "    print(\"the openai_api_key is valid\")\n",
    "else:\n",
    "    print(\"openai key is not valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7ac4f11-216c-4663-9eb4-516da2a19190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Of course! What would you like to request?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request = \"please come up with a challenging, nuanced question that I can ask a number of llms and evaluate their intelligence \"\n",
    "request += \"please answer with the questions, no explanations\"\n",
    "\n",
    "messages = [{\"role\":\"user\", \"content\":\"request\"}]\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "question = response.choices[0].message.content\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfa7177d-1ad0-410c-9226-e51ed2bad33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd be happy to help you with a request. Could you please provide more context or clarify what kind of request you have? Is it related to a specific topic, service, or something else? I'll do my best to assist you.\n"
     ]
    }
   ],
   "source": [
    "#now with ollama\n",
    "\n",
    "request = \"please come up with a challenging, nuanced question that I can ask a number of llms and evaluate their intelligence \"\n",
    "request += \"please answer with the questions, no explanations\"\n",
    "\n",
    "\n",
    "messages = [{\"role\":\"user\", \"content\":\"request\"}]\n",
    "response = ollama.chat(\n",
    "    model = \"llama3.2\",\n",
    "    messages=messages\n",
    ")\n",
    "question = response.message.content\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818e0f47-9f49-4e22-b89e-49a0e7cf3f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with claude how we can ask question and get answer\n",
    "## Anthropic has a slightly different API, and Max Tokens is required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c54e32-31be-4436-9ae1-b6ecb29751b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "claude_api_key = os.getenv('CLAUDE_API_KEY')\n",
    "\n",
    "#now we want to know with claude how we could do this\n",
    "model_name = \"claude-3-7-sonnet-latest\"\n",
    "claude = Anthropic()\n",
    "response = claude.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "answer = response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d56353-3ba3-4be5-bd65-acd5746698c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with gemeni how we caoud do so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77018595-fe6e-40f8-810d-0a6d0f27da12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code do not use openai own model but uses google's gemeni model through openai compatible api format\n",
    "\n",
    "gemeni = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\") #generate gemeni api client using openai format\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "\n",
    "response = gemeni.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02ced743-bac1-429a-9cf4-4c2c254823c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the key is not valid!\n"
     ]
    }
   ],
   "source": [
    "#now we do the same with deepseek\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "deepseek_api_key=os.getenv('DEEPSEEK_API_KEY')\n",
    "if deepseek_api_key:\n",
    "    print('yeah the key exists!')\n",
    "else:\n",
    "    print('the key is not valid!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588b809d-72f8-47e1-a450-7dc1b969fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "model_name = \"deepseek-chat\"\n",
    "response = deepseek.chat.completions.create(model=model_name, messages=messages)\n",
    "response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ea0b17-9188-4d2e-8c61-90624c6e31cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "qrok = OpenAI(api_key=qrok_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "model_name = \"llama-3.3-70b-versatile\"\n",
    "response = qrok.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21962fad-2a2a-4b52-9ddd-a6338af4141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets do this with ollama\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23a1d920-4ac8-4933-ab95-3002da7c182d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why did the data scientist cross the road?\\n\\nTo get to the other side... of the forest, where the data was denser. But honestly, it's just a paws-itive correlation between road crossing and data visualization!\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we want to know how with openai we can use llama3.2\n",
    "ollama = OpenAI(api_key= \"ollama\", base_url=\"http://localhost:11434/v1\")\n",
    "model_name=\"llama3.2\"\n",
    "messages = [{\"role\":\"user\", \"content\":\"tell me a joke about data scientists\"}]\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd00b3f9-5f59-474a-9f1f-dc663f8dfff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nAlright, let\\'s tackle this query. The user wants a joke about machine learning engineers. Hmm, I know they\\'re often seen as innovative and data-driven professionals. So maybe find something funny using their names or roles.\\n\\nThe previous joke mentioned how an engineer might be like a tool that just knows the tools used by another person. That\\'s a classic one about being redundant but effective. Maybe take that angle and add some unique twist on engineers in tech jobs.\\n\\nLet me think. \"You know what I mean...\" could work as the punchline, contrasting the idea of redundancy with their innovative thinking. The joke works because it plays on wordplay and the concept of having a plan that repeats tasks but still being forward-thinking.\\n</think>\\n\\nSure! Here\\'s a joke for you:\\n\\n\"You know what I mean... (waving his hand) You know what a machine learning engineer does in his sleep.\"'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we want to know how with openai we can use deepseek-r1:1.5b\n",
    "ollama = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\n",
    "model_name = \"deepseek-r1:1.5b\"\n",
    "messages = [{\"role\":\"user\", \"content\":\"tell me a joke about machine learning engineers\"}]\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "answer= response.choices[0].message.content\n",
    "answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73e5dc11-591d-43fe-839f-cbafb62b59c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ollama serve  we can not use ollama serve in juoyter  because this tab is for serving and that is it.we should do it in terminal with tmux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0be705c3-f163-4dcd-9e75-396ac077fd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='llama3.2' created_at='2025-07-01T11:00:35.902541679Z' done=True done_reason='stop' total_duration=16775239718 load_duration=16486839781 prompt_eval_count=33 prompt_eval_duration=178346097 eval_count=13 eval_duration=109214160 message=Message(role='assistant', content='The answer to 4 + 5 is 9.', images=None, tool_calls=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The answer to 4 + 5 is 9.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=\"llama3.2\",\n",
    "    messages=[{\"role\":\"user\", \"content\":\"hello what is 4+5?\"}]\n",
    ")\n",
    "print(response)\n",
    "result = response['message']['content']\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd87aae9-a49e-4630-bcd1-b7a23a6706f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "I need to solve the equation 4 plus 5. To do this, I'll add the two numbers together.\n",
      "\n",
      "First, I'll start by identifying the numbers involved: 4 and 5.\n",
      "\n",
      "Next, I'll perform the addition operation on these numbers.\n",
      "\n",
      "After calculating the sum, I'll present the final answer.\n",
      "</think>\n",
      "\n",
      "**Question:**  \n",
      "Hello! What is \\(4 + 5\\)?\n",
      "\n",
      "**Solution:**\n",
      "\n",
      "To find the sum of 4 and 5, follow these steps:\n",
      "\n",
      "1. **Identify the numbers to add:**\n",
      "   \\[\n",
      "   4 \\quad \\text{and} \\quad 5\n",
      "   \\]\n",
      "\n",
      "2. **Add the two numbers together:**\n",
      "   \\[\n",
      "   4 + 5 = 9\n",
      "   \\]\n",
      "\n",
      "**Final Answer:**\n",
      "\\[\n",
      "\\boxed{9}\n",
      "\\]\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='deepseek-r1:1.5b',\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello, what is 4 + 5?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2debd47a-e0c3-4fa3-b94e-12dba9a24daa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
